{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Required Libraries"
      ],
      "metadata": {
        "id": "5tVmnQlzOSc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-09-11T12:10:57.014532Z",
          "iopub.execute_input": "2023-09-11T12:10:57.015112Z",
          "iopub.status.idle": "2023-09-11T12:10:57.367459Z",
          "shell.execute_reply.started": "2023-09-11T12:10:57.015064Z",
          "shell.execute_reply": "2023-09-11T12:10:57.366536Z"
        },
        "trusted": true,
        "id": "HpZLDNMJxI28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install nltk evaluate\n",
        "!pip install datasets rouge_score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:28:45.431344Z",
          "iopub.execute_input": "2023-09-11T13:28:45.432395Z",
          "iopub.status.idle": "2023-09-11T13:29:22.330863Z",
          "shell.execute_reply.started": "2023-09-11T13:28:45.432356Z",
          "shell.execute_reply": "2023-09-11T13:29:22.329667Z"
        },
        "trusted": true,
        "id": "jvnRl4EYxI3B",
        "outputId": "9ccbc354-c5b2-4093-9eac-8b19b762f2f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.33.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.2.4)\nCollecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk) (1.16.0)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.0\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: rouge_score in /opt/conda/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.3.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.9.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.7.22)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from datasets import load_dataset\n",
        "from evaluate import load"
      ],
      "metadata": {
        "id": "gwUkPIG0Oc28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset, metric of evaluation"
      ],
      "metadata": {
        "id": "XDvOKIQpOdoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets = load_dataset(\"vicgalle/alpaca-gpt4\")\n",
        "metric = load(\"rouge\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:39.341503Z",
          "iopub.execute_input": "2023-09-11T13:29:39.341918Z",
          "iopub.status.idle": "2023-09-11T13:29:41.706840Z",
          "shell.execute_reply.started": "2023-09-11T13:29:39.341886Z",
          "shell.execute_reply": "2023-09-11T13:29:41.705924Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "43dd8bf8e8aa4477b17febcefc493184",
            "67e343a0f0a0480db3a052cf16b518ea"
          ]
        },
        "id": "wQ42rTTcxI3D",
        "outputId": "f53b534b-883d-4763-f75c-42c657bafd42"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43dd8bf8e8aa4477b17febcefc493184"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67e343a0f0a0480db3a052cf16b518ea"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"vicgalle/alpaca-gpt4\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T12:11:37.522077Z",
          "iopub.execute_input": "2023-09-11T12:11:37.523018Z",
          "iopub.status.idle": "2023-09-11T12:11:39.927717Z",
          "shell.execute_reply.started": "2023-09-11T12:11:37.522969Z",
          "shell.execute_reply": "2023-09-11T12:11:39.926558Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "24be7156181d4365a251cec63fc8f558",
            "40eb05ed386d4d3fbf8f57e7f72c57f8",
            "79de7338941c451cb8e3e636e91f082d",
            "f36b5026efaf453d9cf70338a77b3c08"
          ]
        },
        "id": "tRNQo75OxI3E",
        "outputId": "726f20ed-43eb-48c1-a0c6-b51f73433fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Downloading and preparing dataset parquet/vicgalle--alpaca-gpt4 to /root/.cache/huggingface/datasets/parquet/vicgalle--alpaca-gpt4-1e85e31ce0639161/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24be7156181d4365a251cec63fc8f558"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading data:   0%|          | 0.00/48.4M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40eb05ed386d4d3fbf8f57e7f72c57f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79de7338941c451cb8e3e636e91f082d"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/vicgalle--alpaca-gpt4-1e85e31ce0639161/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f36b5026efaf453d9cf70338a77b3c08"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PreProcessing"
      ],
      "metadata": {
        "id": "jjq6V29ROnMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_instruction_input(example):\n",
        "    example[\"instruction\"] = example[\"instruction\"] + \" \" + example[\"input\"]\n",
        "    return {\"output\": example[\"output\"], \"text\": example[\"text\"]}\n",
        "\n",
        "# Apply the function to the dataset and add the new \"combined_input\" column\n",
        "dataset = dataset.map(combine_instruction_input)\n",
        "\n",
        "# Display the updated dataset\n",
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T12:11:39.929325Z",
          "iopub.execute_input": "2023-09-11T12:11:39.929967Z",
          "iopub.status.idle": "2023-09-11T12:11:45.351585Z",
          "shell.execute_reply.started": "2023-09-11T12:11:39.929926Z",
          "shell.execute_reply": "2023-09-11T12:11:45.350637Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "698f5ee8842c42948f69654d16b76c67"
          ]
        },
        "id": "K6x8fSSGxI3F",
        "outputId": "aae300a6-75b2-4690-8c6e-071fa6abfe6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/52002 [00:00<?, ?ex/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "698f5ee8842c42948f69654d16b76c67"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'output', 'text'],\n        num_rows: 52002\n    })\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5TokenizerFast, T5ForQuestionAnswering, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
        "\n",
        "\n",
        "tokenizer = T5TokenizerFast.from_pretrained(\"google/t5-efficient-tiny\")\n",
        "model = T5ForQuestionAnswering.from_pretrained(\"google/t5-efficient-tiny\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T12:11:45.353235Z",
          "iopub.execute_input": "2023-09-11T12:11:45.353634Z",
          "iopub.status.idle": "2023-09-11T12:12:00.325406Z",
          "shell.execute_reply.started": "2023-09-11T12:11:45.353592Z",
          "shell.execute_reply": "2023-09-11T12:12:00.324378Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "9ebe360e952d45419c869fc7e08df3c9",
            "3de6c4b86ef945b6b2b5932f993aed1e",
            "16388f3b8fe948108c87fa6ce53a3052",
            "335494affbcb4dcab48ddd1a4d5c36a4",
            "7b6f16664ca54c368efb1071aa7d4252",
            "0a21ab288bb04d1db641475b4844360c"
          ]
        },
        "id": "rw54kJlyxI3G",
        "outputId": "5b300e18-87a6-4e72-9fa8-ed90dadef6cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)okenizer_config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ebe360e952d45419c869fc7e08df3c9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3de6c4b86ef945b6b2b5932f993aed1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16388f3b8fe948108c87fa6ce53a3052"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "335494affbcb4dcab48ddd1a4d5c36a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading (…)lve/main/config.json:   0%|          | 0.00/628 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b6f16664ca54c368efb1071aa7d4252"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/62.3M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a21ab288bb04d1db641475b4844360c"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of T5ForQuestionAnswering were not initialized from the model checkpoint at google/t5-efficient-tiny and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the train set\n",
        "dataset[\"train\"] = dataset[\"train\"].shuffle()\n",
        "\n",
        "# Split the shuffled train set into train, test, and validation sets\n",
        "datasets_train_test = dataset[\"train\"].train_test_split(test_size=5000)\n",
        "datasets_train_validation = datasets_train_test[\"train\"].train_test_split(test_size=5000)\n",
        "\n",
        "# Assign the split sets to the data dictionary\n",
        "dataset[\"train\"] = datasets_train_validation[\"train\"]\n",
        "dataset[\"validation\"] = datasets_train_validation[\"test\"]\n",
        "dataset[\"test\"] = datasets_train_test[\"test\"]\n",
        "\n",
        "# Print the sizes of the train, test, and validation sets\n",
        "print(f\"The size of the train set is {len(dataset['train'])}\")\n",
        "print(f\"The size of the test set is {len(dataset['test'])}\")\n",
        "print(f\"The size of the validation set is {len(dataset['validation'])}\")\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T12:12:00.328586Z",
          "iopub.execute_input": "2023-09-11T12:12:00.328881Z",
          "iopub.status.idle": "2023-09-11T12:12:00.413442Z",
          "shell.execute_reply.started": "2023-09-11T12:12:00.328856Z",
          "shell.execute_reply": "2023-09-11T12:12:00.412532Z"
        },
        "trusted": true,
        "id": "pctv68HvxI3I",
        "outputId": "9c31ef5d-7161-48bb-8dba-3da46f580ac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "The size of the train set is 42002\nThe size of the test set is 5000\nThe size of the validation set is 5000\n",
          "output_type": "stream"
        },
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'output', 'text'],\n        num_rows: 42002\n    })\n    validation: Dataset({\n        features: ['instruction', 'input', 'output', 'text'],\n        num_rows: 5000\n    })\n    test: Dataset({\n        features: ['instruction', 'input', 'output', 'text'],\n        num_rows: 5000\n    })\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(dataset)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T12:12:00.414762Z",
          "iopub.execute_input": "2023-09-11T12:12:00.415740Z",
          "iopub.status.idle": "2023-09-11T12:12:00.421768Z",
          "shell.execute_reply.started": "2023-09-11T12:12:00.415704Z",
          "shell.execute_reply": "2023-09-11T12:12:00.420825Z"
        },
        "trusted": true,
        "id": "nR-NzUwvxI3K",
        "outputId": "685389f2-f59d-4f1f-d33c-83c2e03a664d"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "datasets.dataset_dict.DatasetDict"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"answer_question: \"\n",
        "max_input_length = 512\n",
        "max_target_length = 512\n",
        "\n",
        "def clean_text(text):\n",
        "  sentences = nltk.sent_tokenize(text.strip())\n",
        "  sentences_cleaned = [s for sent in sentences for s in sent.split(\"\\n\")]\n",
        "  sentences_cleaned_no_titles = [sent for sent in sentences_cleaned\n",
        "                                 if len(sent) > 0 and\n",
        "                                 sent[-1] in string.punctuation]\n",
        "  text_cleaned = \"\\n\".join(sentences_cleaned_no_titles)\n",
        "  return text_cleaned\n",
        "\n",
        "def preprocess_data(examples):\n",
        "  texts_cleaned = [clean_text(text) for text in examples[\"instruction\"]]\n",
        "  inputs = [prefix + text for text in examples[\"instruction\"]]\n",
        "  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding=True)\n",
        "\n",
        "  # Setup the tokenizer for targets\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(examples[\"output\"], max_length=max_target_length,padding=True ,\n",
        "                       truncation=True)\n",
        "\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "  return model_inputs"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T12:12:00.423356Z",
          "iopub.execute_input": "2023-09-11T12:12:00.424028Z",
          "iopub.status.idle": "2023-09-11T12:12:00.435530Z",
          "shell.execute_reply.started": "2023-09-11T12:12:00.423996Z",
          "shell.execute_reply": "2023-09-11T12:12:00.434554Z"
        },
        "trusted": true,
        "id": "7DAfO7dSxI3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "tokenized_datasets = dataset.map(preprocess_data, batched=True)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T12:12:59.120068Z",
          "iopub.execute_input": "2023-09-11T12:12:59.120507Z",
          "iopub.status.idle": "2023-09-11T12:13:37.973509Z",
          "shell.execute_reply.started": "2023-09-11T12:12:59.120457Z",
          "shell.execute_reply": "2023-09-11T12:13:37.972419Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "955805bb0941415e8ad622d2f2da7201",
            "96f12f1ca53d4d2ca43696dba3e8471c",
            "148392261aba4c8f8b24b5cc8c4d9b1a"
          ]
        },
        "id": "4ufDANLsxI3M",
        "outputId": "d01c5b2c-6823-40a2-a664-ae0f4935a1b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/43 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "955805bb0941415e8ad622d2f2da7201"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/5 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "96f12f1ca53d4d2ca43696dba3e8471c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/5 [00:00<?, ?ba/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "148392261aba4c8f8b24b5cc8c4d9b1a"
            }
          },
          "metadata": {}
        },
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'output', 'text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 42002\n    })\n    validation: Dataset({\n        features: ['instruction', 'input', 'output', 'text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 5000\n    })\n    test: Dataset({\n        features: ['instruction', 'input', 'output', 'text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 5000\n    })\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T12:13:37.975322Z",
          "iopub.execute_input": "2023-09-11T12:13:37.976145Z",
          "iopub.status.idle": "2023-09-11T12:13:37.984305Z",
          "shell.execute_reply.started": "2023-09-11T12:13:37.976110Z",
          "shell.execute_reply": "2023-09-11T12:13:37.983283Z"
        },
        "trusted": true,
        "id": "l4gCt2lCxI3N",
        "outputId": "5558b8ac-da70-4f11-aa0c-e4e79edb5669"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 12,
          "output_type": "execute_result",
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['instruction', 'input', 'output', 'text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 42002\n    })\n    validation: Dataset({\n        features: ['instruction', 'input', 'output', 'text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 5000\n    })\n    test: Dataset({\n        features: ['instruction', 'input', 'output', 'text', 'input_ids', 'attention_mask', 'labels'],\n        num_rows: 5000\n    })\n})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "        # Define the directory path where you want to save the model\n",
        "dirr = \"/kaggle/working/my_model_directory/\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs(dirr, exist_ok=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T12:13:37.985734Z",
          "iopub.execute_input": "2023-09-11T12:13:37.986341Z",
          "iopub.status.idle": "2023-09-11T12:13:38.069445Z",
          "shell.execute_reply.started": "2023-09-11T12:13:37.986285Z",
          "shell.execute_reply": "2023-09-11T12:13:38.068268Z"
        },
        "trusted": true,
        "id": "7wBaVopxxI3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T13:29:53.620725Z",
          "iopub.execute_input": "2023-09-11T13:29:53.621102Z",
          "iopub.status.idle": "2023-09-11T13:29:53.628160Z",
          "shell.execute_reply.started": "2023-09-11T13:29:53.621072Z",
          "shell.execute_reply": "2023-09-11T13:29:53.626954Z"
        },
        "trusted": true,
        "id": "kUV5-DPwxI3O",
        "outputId": "d2143649-00f4-45c6-f376-beb8a8a2febd"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 65,
          "output_type": "execute_result",
          "data": {
            "text/plain": "EvaluationModule(name: \"rouge\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\nCalculates average rouge scores for a list of hypotheses and references\nArgs:\n    predictions: list of predictions to score. Each prediction\n        should be a string with tokens separated by spaces.\n    references: list of reference for each prediction. Each\n        reference should be a string with tokens separated by spaces.\n    rouge_types: A list of rouge types to calculate.\n        Valid names:\n        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n        `\"rougeL\"`: Longest common subsequence based scoring.\n        `\"rougeLsum\"`: rougeLsum splits text using `\"\n\"`.\n        See details in https://github.com/huggingface/datasets/issues/617\n    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n    use_aggregator: Return aggregates if this is set to True\nReturns:\n    rouge1: rouge_1 (f1),\n    rouge2: rouge_2 (f1),\n    rougeL: rouge_l (f1),\n    rougeLsum: rouge_lsum (f1)\nExamples:\n\n    >>> rouge = evaluate.load('rouge')\n    >>> predictions = [\"hello there\", \"general kenobi\"]\n    >>> references = [\"hello there\", \"general kenobi\"]\n    >>> results = rouge.compute(predictions=predictions, references=references)\n    >>> print(results)\n    {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n\"\"\", stored examples: 0)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning"
      ],
      "metadata": {
        "id": "PFrlD5R8PNHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "model_dir = f\"/kaggle/working\"\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    model_dir,\n",
        "    save_total_limit = 5,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=200,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    learning_rate=0.002,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "#     metric_for_best_model=\"rogue1\",\n",
        "    report_to=\"tensorboard\"\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T14:10:31.237415Z",
          "iopub.execute_input": "2023-09-11T14:10:31.237861Z",
          "iopub.status.idle": "2023-09-11T14:10:31.244970Z",
          "shell.execute_reply.started": "2023-09-11T14:10:31.237828Z",
          "shell.execute_reply": "2023-09-11T14:10:31.243984Z"
        },
        "trusted": true,
        "id": "67ItA2IQxI3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T14:10:31.605931Z",
          "iopub.execute_input": "2023-09-11T14:10:31.606626Z",
          "iopub.status.idle": "2023-09-11T14:10:31.611524Z",
          "shell.execute_reply.started": "2023-09-11T14:10:31.606588Z",
          "shell.execute_reply": "2023-09-11T14:10:31.610536Z"
        },
        "trusted": true,
        "id": "AemjyT1sxI3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T14:10:31.961040Z",
          "iopub.execute_input": "2023-09-11T14:10:31.961376Z",
          "iopub.status.idle": "2023-09-11T14:10:31.968531Z",
          "shell.execute_reply.started": "2023-09-11T14:10:31.961350Z",
          "shell.execute_reply": "2023-09-11T14:10:31.967312Z"
        },
        "trusted": true,
        "id": "8CpBoUNzxI3P",
        "outputId": "c96aa5dc-d0ee-46b0-d30f-96d2603074cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 110,
          "output_type": "execute_result",
          "data": {
            "text/plain": "EvaluationModule(name: \"rouge\", module_type: \"metric\", features: [{'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id=None)}, {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}], usage: \"\"\"\nCalculates average rouge scores for a list of hypotheses and references\nArgs:\n    predictions: list of predictions to score. Each prediction\n        should be a string with tokens separated by spaces.\n    references: list of reference for each prediction. Each\n        reference should be a string with tokens separated by spaces.\n    rouge_types: A list of rouge types to calculate.\n        Valid names:\n        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n        `\"rougeL\"`: Longest common subsequence based scoring.\n        `\"rougeLsum\"`: rougeLsum splits text using `\"\n\"`.\n        See details in https://github.com/huggingface/datasets/issues/617\n    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n    use_aggregator: Return aggregates if this is set to True\nReturns:\n    rouge1: rouge_1 (f1),\n    rouge2: rouge_2 (f1),\n    rougeL: rouge_l (f1),\n    rougeLsum: rouge_lsum (f1)\nExamples:\n\n    >>> rouge = evaluate.load('rouge')\n    >>> predictions = [\"hello there\", \"general kenobi\"]\n    >>> references = [\"hello there\", \"general kenobi\"]\n    >>> results = rouge.compute(predictions=predictions, references=references)\n    >>> print(results)\n    {'rouge1': 1.0, 'rouge2': 1.0, 'rougeL': 1.0, 'rougeLsum': 1.0}\n\"\"\", stored examples: 0)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "    predictions = eval_pred.predictions\n",
        "    labels = eval_pred.label_ids\n",
        "\n",
        "    # Decode the predictions and labels\n",
        "    decoded_preds = trainer.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = trainer.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    rouge_scores = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    print(rouge_scores)\n",
        "\n",
        "#     # Extract ROUGE f1 scores\n",
        "#     rouge_scores = {key: value[\"f\"] * 100 for key, value in rouge_scores.items()}\n",
        "\n",
        "#     # Compute mean generated length\n",
        "#     prediction_lens = [np.count_nonzero(pred != trainer.tokenizer.pad_token_id) for pred in predictions]\n",
        "#     mean_gen_len = np.mean(prediction_lens)\n",
        "\n",
        "#     # Add mean generated length to metrics\n",
        "#     rouge_scores[\"gen_len\"] = mean_gen_len\n",
        "\n",
        "    return rouge_scores"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T14:10:32.185604Z",
          "iopub.execute_input": "2023-09-11T14:10:32.185897Z",
          "iopub.status.idle": "2023-09-11T14:10:32.192307Z",
          "shell.execute_reply.started": "2023-09-11T14:10:32.185872Z",
          "shell.execute_reply": "2023-09-11T14:10:32.191291Z"
        },
        "trusted": true,
        "id": "8ROZJj_axI3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/t5-efficient-tiny\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T14:10:32.505861Z",
          "iopub.execute_input": "2023-09-11T14:10:32.506796Z",
          "iopub.status.idle": "2023-09-11T14:10:33.036724Z",
          "shell.execute_reply.started": "2023-09-11T14:10:32.506762Z",
          "shell.execute_reply": "2023-09-11T14:10:33.035697Z"
        },
        "trusted": true,
        "id": "5G9KpOmXxI3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T14:10:33.038766Z",
          "iopub.execute_input": "2023-09-11T14:10:33.039131Z",
          "iopub.status.idle": "2023-09-11T14:10:33.070334Z",
          "shell.execute_reply.started": "2023-09-11T14:10:33.039097Z",
          "shell.execute_reply": "2023-09-11T14:10:33.069411Z"
        },
        "trusted": true,
        "id": "YHM6_afDxI3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T14:10:33.711787Z",
          "iopub.execute_input": "2023-09-11T14:10:33.712529Z",
          "iopub.status.idle": "2023-09-11T14:52:27.519757Z",
          "shell.execute_reply.started": "2023-09-11T14:10:33.712471Z",
          "shell.execute_reply": "2023-09-11T14:52:27.518567Z"
        },
        "trusted": true,
        "id": "CcJ8q2_oxI3R",
        "outputId": "0cafd163-ddf3-4e4f-eadb-72da711e579d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='2626' max='2626' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2626/2626 41:52, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rouge1</th>\n      <th>Rouge2</th>\n      <th>Rougel</th>\n      <th>Rougelsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>1.637500</td>\n      <td>1.195163</td>\n      <td>0.188658</td>\n      <td>0.082730</td>\n      <td>0.166786</td>\n      <td>0.166910</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.352700</td>\n      <td>1.154613</td>\n      <td>0.189255</td>\n      <td>0.085125</td>\n      <td>0.167547</td>\n      <td>0.167405</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.328100</td>\n      <td>1.135040</td>\n      <td>0.205986</td>\n      <td>0.099838</td>\n      <td>0.182294</td>\n      <td>0.182289</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.292700</td>\n      <td>1.118666</td>\n      <td>0.209481</td>\n      <td>0.104214</td>\n      <td>0.185747</td>\n      <td>0.185619</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.268900</td>\n      <td>1.108846</td>\n      <td>0.213118</td>\n      <td>0.108022</td>\n      <td>0.189805</td>\n      <td>0.189585</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.247000</td>\n      <td>1.101533</td>\n      <td>0.219177</td>\n      <td>0.112388</td>\n      <td>0.195390</td>\n      <td>0.195170</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.260400</td>\n      <td>1.090449</td>\n      <td>0.219545</td>\n      <td>0.111865</td>\n      <td>0.194772</td>\n      <td>0.194625</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.243200</td>\n      <td>1.085774</td>\n      <td>0.222310</td>\n      <td>0.115640</td>\n      <td>0.197728</td>\n      <td>0.197428</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.208600</td>\n      <td>1.078656</td>\n      <td>0.222588</td>\n      <td>0.114535</td>\n      <td>0.197969</td>\n      <td>0.197783</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.218500</td>\n      <td>1.073141</td>\n      <td>0.224987</td>\n      <td>0.116378</td>\n      <td>0.200044</td>\n      <td>0.199958</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>1.229700</td>\n      <td>1.070143</td>\n      <td>0.226058</td>\n      <td>0.117257</td>\n      <td>0.201230</td>\n      <td>0.201145</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.226900</td>\n      <td>1.066998</td>\n      <td>0.227303</td>\n      <td>0.118131</td>\n      <td>0.202161</td>\n      <td>0.202074</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>1.164200</td>\n      <td>1.064905</td>\n      <td>0.227955</td>\n      <td>0.117926</td>\n      <td>0.202703</td>\n      <td>0.202620</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "{'rouge1': 0.1886581710879507, 'rouge2': 0.08272986218422441, 'rougeL': 0.16678559660476994, 'rougeLsum': 0.16691045551547118}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "{'rouge1': 0.18925491054581278, 'rouge2': 0.08512455010249378, 'rougeL': 0.16754661752031103, 'rougeLsum': 0.16740476702389606}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "{'rouge1': 0.20598590068466505, 'rouge2': 0.09983755992378326, 'rougeL': 0.18229364572911527, 'rougeLsum': 0.18228939666500366}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "{'rouge1': 0.2094814715258595, 'rouge2': 0.10421437243692258, 'rougeL': 0.1857472064207975, 'rougeLsum': 0.18561943805346204}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "{'rouge1': 0.21311825973656554, 'rouge2': 0.10802155645150047, 'rougeL': 0.18980489239060022, 'rougeLsum': 0.18958453371246775}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "{'rouge1': 0.21917740669353275, 'rouge2': 0.11238846313326412, 'rougeL': 0.19539008551851045, 'rougeLsum': 0.19516969905758103}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "{'rouge1': 0.2195451328090618, 'rouge2': 0.11186546013706714, 'rougeL': 0.19477159069887554, 'rougeLsum': 0.1946247682505859}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "{'rouge1': 0.22230979591018923, 'rouge2': 0.11564033616189957, 'rougeL': 0.1977277633343446, 'rougeLsum': 0.1974275660688038}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "{'rouge1': 0.22258779860887262, 'rouge2': 0.11453516895173499, 'rougeL': 0.19796947397255132, 'rougeLsum': 0.19778328162594472}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "{'rouge1': 0.22498729753258745, 'rouge2': 0.11637770864061117, 'rougeL': 0.2000442618809512, 'rougeLsum': 0.1999576218802684}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "{'rouge1': 0.22605799888172828, 'rouge2': 0.11725722906300134, 'rougeL': 0.20123003627832914, 'rougeLsum': 0.2011448052850919}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "{'rouge1': 0.2273029149718893, 'rouge2': 0.11813148242841814, 'rougeL': 0.2021610633197315, 'rougeLsum': 0.20207434865826318}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "{'rouge1': 0.22795511370635466, 'rouge2': 0.1179258663478244, 'rougeL': 0.20270301820940667, 'rougeLsum': 0.20262043572615437}\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "execution_count": 114,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=2626, training_loss=1.2809525996060744, metrics={'train_runtime': 2513.1041, 'train_samples_per_second': 16.713, 'train_steps_per_second': 1.045, 'total_flos': 714026131113984.0, 'train_loss': 1.2809525996060744, 'epoch': 1.0})"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the last saved model for predictions"
      ],
      "metadata": {
        "id": "WaAQOSo3PTdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = \"t5-efficient-tiny/checkpoint-200\"\n",
        "model_dir = '/kaggle/working/checkpoint-2600'\n",
        "\n",
        "tokenizer = T5TokenizerFast.from_pretrained(model_dir)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
        "\n",
        "max_input_length = 512"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T15:00:07.256237Z",
          "iopub.execute_input": "2023-09-11T15:00:07.256695Z",
          "iopub.status.idle": "2023-09-11T15:00:07.620875Z",
          "shell.execute_reply.started": "2023-09-11T15:00:07.256659Z",
          "shell.execute_reply": "2023-09-11T15:00:07.619876Z"
        },
        "trusted": true,
        "id": "dzycUOu4xI3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Write a story on moon\"\"\"\n",
        "\n",
        "inputs = [\"answer_question \" + text]\n",
        "\n",
        "inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
        "output = model.generate(**inputs, do_sample=True, min_length=10, max_length=64)\n",
        "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "predicted_title = nltk.sent_tokenize(decoded_output.strip())[0]\n",
        "\n",
        "# Conversational AI: The Future of Customer Service\n",
        "predicted_title"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-11T15:22:29.606329Z",
          "iopub.execute_input": "2023-09-11T15:22:29.606717Z",
          "iopub.status.idle": "2023-09-11T15:22:30.348380Z",
          "shell.execute_reply.started": "2023-09-11T15:22:29.606685Z",
          "shell.execute_reply": "2023-09-11T15:22:30.347395Z"
        },
        "trusted": true,
        "id": "IIh7YF-xxI3S",
        "outputId": "15b31142-4640-43d3-a3c5-f1c4b10fe321"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 128,
          "output_type": "execute_result",
          "data": {
            "text/plain": "'Once upon a time, in a circle of dawn when the moon suddenly arrived, the moon was in and the moon was a mystical and powerful moon of the moon.'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sL3VwCyKxI3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7l2C6dTYxI3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RfHlsPs2xI3T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}